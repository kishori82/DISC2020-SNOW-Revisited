\section{Introduction}
\label{sec:intro}
% what are distributed storage systems; why they are important.
Today's web services are built on \textit{distributed storage systems} that provide fault tolerant and scalable access to data.
%, i.e., a fundamental building block for large-scale applications such as Facebook, Google, and Amazon, which have a great impact on people's everyday life.
Distributed storage systems scale their capacity and throughput by \textit{sharding} (i.e., partitioning) data across many machines within a datacenter, i.e., each machine stores a subset of the data.
They also \textit{geo-replicate} the data across several geographically dispersed datacenters to tolerate failures and to increase their proximity to users.

% consistency and performance tradeoff
Distributed storage systems abstract away the complexities of sharding and replication from application code by providing guarantees for accesses to data.
These \textit{guarantees} include consistency and transactions.
\textit{Consistency} controls the values of data that accesses may observe
and \textit{transactions} dictate what accesses may be grouped together.
Stronger guarantees provide an abstraction closer to a single-threaded environment, greatly simplifying application code.
Ensuring the guarantees hold, however, often comes with worse performance.
Therefore, the tradeoff between performance and guarantees lies at the heart of designing such systems.

%% Despite their distributed nature, these systems provide guarantees on \textit{consistency}, which arbitrates the commit order of distributed operations: stronger consistency provides an abstraction closer to a single-threaded environment, greatly simplifying application code. However, stronger consistency often comes with worse performance due to the overhead in coordinating an order among operations. Therefore, the tradeoff between performance and consistency lies at the heart of designing such systems.

% earlier work on replication
The performance-guarantee tradeoffs that result from replication have been well-studied with several well-known impossibility results~\cite{AW94, Fischer:pds1983,Gilbert:sigact2002, lipton88pram}.
For instance, the CAP Theorem~\cite{Gilbert:sigact2002} proves that system designers must choose either availability during network partitions (performance) or strong consistency across replicas (guarantee).
However, little prior work exists on what performance-guarantee tradeoffs result from sharding.
%(splitting datasets across multiple servers).
%, i.e., within the scope of a datacenter across hundreds of thousands of machines.

Understanding the performance-guarantee tradeoff due to sharding is important because user requests are typically handled across many shards but within a single nearby datacenter (replica).
This is particularly true for the reads needed to handle a user request, which are what dominate real-world workloads:
Facebook reported 500 reads for every write in their TAO system~\cite{TAO2013} and
Google reported three orders of magnitude more reads than general transactions for their F1 database that runs on their Spanner system~\cite{Corbett:osdi2012}.
In this work we focus on clarifying the performance-guarantee tradeoff for reads that results from sharding.
%, an impossibility result proving that there exists a fundamental tradeoff between the latency of {\
%  \sc READ} transactions and the consistency systems provide.
% SNOW
%\subsection{\roT{} Guarantees and Latency}
Distributed storage systems group read requests (that each individually access a separate shard) into \textit{\rots{}} that together return a consistent, cross-shard view of the system.
Whether a view is consistent is determined by the consistency model a system provides.
The ideal \rots{} would have the strongest guarantees:
They would provide strict serializability~\cite{Papadimitriou79}, the strongest consistency model,
and they could be used in a system that also includes \textit{\wots{}} that group write requests (each to a separate shard) together.
The alternative to the latter property are \rots{} that can only be used in systems that have non-transactional, \textit{simple writes}.

The ideal \rots{} would also provide the best performance.
In particular, they would provide the lowest possible latency because the prevalence of reads makes them dominate the user response times that are aggressively optimized by web services~\cite{latency:shopzilla, latency:amazon,
  latency:search}.
The \textit{optimal latency} for a \rot{} is to match the latency of non-transactional, \textit{simple reads}:
complete in a single round trip of non-blocking parallel requests to the shards that return only the requested data~\cite{SNOW2016}.

%To the best of our knowledge, the only result in the sharding dimension that is relevant to \rots{} is the SNOW Theorem~\cite{SNOW2016}.
\subsection{Previous Results and Open Questions}
The SNOW Theorem was the first result in the sharding dimension that is relevant to \rots{}~\cite{SNOW2016}.
The SNOW Theorem is an impossibility result that proves no \rot{} can provide \textbf{S}trict serializability with \textbf{N}on-blocking client-server communication that completes with \textbf{O}ne response per read in a system with \wotsSNOW{} (\S\ref{sec:snow}).
It shows there is a fundamental tradeoff between the latency and guarantees of \rots{} that system designers must grapple with, they must pick either the strongest guarantees (S and W) or optimal latency (N and O).
%Section~\ref{sec:snow} defines the four SNOW properties more precisely.

SNOW is trivially possible in systems with a single client or a single server because the single entity naturally serializes all transactions.
The SNOW Theorem shows SNOW is impossible in systems with at least three clients and at least two servers.
It explicitly leaves open the question of the possibility of SNOW in a system with two clients.
{\color{blue}
In addition, the model used in the prior work implicitly leaves open several questions.
It assumed the three clients were a single writer and multiple readers (SWMR).
This leaves open the possibility of SNOW with multiple writers and a single reader (MWSR).
The SNOW Theorem also implicitly assumed that clients do not directly exchange messages and that write operations in such a system must eventually complete.
%disallowed client-to-client communication (C2C).
This also leaves open the question of whether allowing or disallowing client-to-client (C2C) communication has any impact on the feasibility of \rot{}s with SNOW properties.
}

In this work, the new impossibility results % built upon the SNOW Theorem 
are philosophically similar to other impossibility results---such as FLP~\cite{Fischer:pds1983} and CAP~\cite{Brewer:pdc2000, Gilbert:sigact2002}---in that they help system designers avoid wasting effort in trying to achieve the impossible. That is, the SNOW Theorem identifies a boundary in the design space of \rots{}, beyond which no algorithms can possibly exist. By revisiting SNOW, our work makes this boundary more precise.


% was 
% First, we state the SNOW properties in the context ofÂ execution, as in I/O automata~\cite{Lynch1996},

\subsection{Our Contributions}
{\color{blue}
Our work builds on the SNOW Theorem to clarify this fundamental tradeoff by providing a thorough proof for the SNOW Theorem~\cite{SNOW2016} and also, answer the open questions mentioned above. 
First, we formally state the SNOW properties in the context of executions of I/O automata~\cite{Lynch1996}  with the additional requirement, for the W property, that any WRITE must eventually complete (\S\ref{sec:prelim}). Next, we identify and prove some basic results, required in the proofs, for transforming one valid execution to another possible and safe execution in a \rot{} system (\S\ref{sec:summary}). 
Then we present a new, rigorous proof of the impossibility of SNOW with three or more clients even when client-to-client (C2C) communication is allowed (\S\ref{sec:formal_proof}).
Next, we show that the feasibility of implementing an algorithm with SNOW in MWSR (which also includes the two-client system model) depends on whether C2C communication is allowed:
when it is not allowed, SNOW is impossible (\S\ref{subsec:no_snow_no_c2c}); and when it is allowed, SNOW is possible (\S\ref{app:algorithm-a}) for any MWSR setting.

Prior to this work, the Eiger~\cite{Lloyd:nsdi2013} algorithm was previously believed to be the only algorithm that provided a bounded number of non-blocking rounds~\cite{Lloyd:nsdi2013} and guaranteed strict serializability. 
Next, we show this claim is not true by showing that not all execution of Eiger is strictly serializable (\S\ref{sec:eiger}).

Next, after realizing the limits posed by the SNOW Theorem, we ask ourselves whether it is possible to construct \rot{} algorithms with no C2C communication, as in most practical systems, where one of the SNOW properties is relaxed. One obvious candidate property is the ``O" property, where one of the two restrictions (i.e., ``one-round" of communication and ``one-version" of data) can be relaxed. We provide two algorithms for the \emph{multiple-writers multi-reader} (MWMR) setting: the first algorithm $B$ guarantees SNW and the ``one-version" property and completes \rot{}s in two rounds (\S\ref{mwmr_snow_one_version}); the second algorithm, $C$, guarantees SNW and the ``one-round" property but returns up to as many versions of the data as there are concurrent \wot{}s
(\S\ref{mwmr_snow_one_round}). Thereby, making these \rot{}s algorithms with a bounded number of non-blocking rounds and guarantees strict serializability. 
%
Due to space limitations, we omit most of the proofs and present them in an extended version in arXiv~\cite{konwar2018snow}.
}

\remove{ KMK
The general impossibility of SNOW, even with client-to-client communication, raises the question of what the best achievable latency is for \rot{}s with the strongest guarantees (S and W).
Surprisingly, there are no existing algorithms with bounded latency.
All algorithms either have an unbounded amount of blocking~\cite{Mu:osdi2016, Thomson:sigmod2012, Mu:osdi2014} or take an unbounded number of rounds~\cite{Wei:sosp2015, Lee:sosp2015, Aguilera:sosp2007}.
One algorithm was previously believed to provide a bounded number of non-blocking rounds~\cite{Lloyd:nsdi2013}, but we show that it does not provide strict serializability (\S\ref{sec:eiger}).

We present the first \rot{} algorithms with bounded latency that can be used in systems with the strongest guarantees.
Figure~\ref{fig:contr_table_b} summarizes what our new algorithms achieve.
Our algorithms all use only non-blocking operations, making them SNW algorithms.
The one response per read (O) property requires one round trip from the client to the servers and that the responses contain only a single version of each requested object.
While providing both parts of the O property is impossible for SNW algorithms, we show that either part is individually possible.

Our first algorithm returns one version of each requested data item and finishes in two rounds (\S\ref{mwmr_snow_one_version}).
Our second algorithm finishes in one round and returns $W$ versions, where $W$ is the number of ongoing \wots{} (\S\ref{mwmr_snow_one_round}).
%Our third algorithm always completes with two or fewer versions returned in two or fewer rounds with its best case completing in a single round with single versions (\S\ref{mwmr_snow_one_round}).
%This final algorithm thus provides SNOW in its best case and remains close to SNOW with only an extra round and version in its worst case.
We prove that our algorithms are non-blocking, strictly serializable, and respect write isolation.
}

\input{contr-table}
